{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjiZZplfKGHB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPlnaCzrZvel"
      },
      "outputs": [],
      "source": [
        "!pip install wfdb\n",
        "!pip install scikit-learn\n",
        "!pip install vmdpy\n",
        "!pip install pywavelets\n",
        "!pip install pyunpack patool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLAR8S8cK7sk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, MaxPooling1D, LSTM, Bidirectional, Reshape, Concatenate, Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from scipy.signal import find_peaks, butter, filtfilt, welch\n",
        "from scipy.stats import entropy\n",
        "import pywt\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from scipy.signal import lfilter\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.saving import register_keras_serializable\n",
        "from pyunpack import Archive\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.utils import class_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BLCHp4oaB28"
      },
      "source": [
        "#Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4axrtR8MkgCM"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/Colab Notebooks/An open-access arrhythmia database of wearable dynamic electrocardiogram.rar\"\n",
        "extract_path = \"/content/drive/MyDrive/Colab Notebooks/Extracted/\"\n",
        "\n",
        "if not os.path.exists(extract_path) or not os.listdir(extract_path):\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "    Archive(base_path).extractall(extract_path)\n",
        "    print(f\"Data extracted to {extract_path}\")\n",
        "else:\n",
        "    print(f\"Data already exists in the directory: {extract_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtzKYKAH9AWO"
      },
      "outputs": [],
      "source": [
        "# Function to load the database\n",
        "def load_ecg_database(base_path):\n",
        "    database = {\"A\": [], \"N\": [], \"V\": []}\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.mat'):  # Process only `.mat` files\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Extract main folder (e.g., \"A\", \"N\", \"V\")\n",
        "                main_folder = file_path.split('/')[-4]\n",
        "                if main_folder in database:\n",
        "                    try:\n",
        "                        mat_data = loadmat(file_path)\n",
        "                        database[main_folder].append((file, mat_data))\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading file {file_path}: {e}\")\n",
        "    return database\n",
        "\n",
        "# Low-pass filter function\n",
        "def butter_lowpass_filter(data, cutoff, fs, order=4):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = cutoff / nyquist\n",
        "    b, a = butter(order, low, btype='low')\n",
        "    y = filtfilt(b, a, data)\n",
        "    return y\n",
        "\n",
        "# High-pass filter function\n",
        "def butter_highpass_filter(data, cutoff, fs, order=4):\n",
        "    nyquist = 0.5 * fs\n",
        "    high = cutoff / nyquist\n",
        "    b, a = butter(order, high, btype='high')\n",
        "    y = filtfilt(b, a, data)\n",
        "    return y\n",
        "\n",
        "# Function for wavelet-based filtering\n",
        "def wavelet_filter(signal, wavelet='db6', level=5):\n",
        "    \"\"\"\n",
        "    Filtering the signal using wavelet transform.\n",
        "    \"\"\"\n",
        "    coeffs = pywt.wavedec(signal, wavelet=wavelet, level=level)\n",
        "\n",
        "    # Zeroing out high-frequency noise components\n",
        "    for i in range(1, len(coeffs)):\n",
        "        coeffs[i] = pywt.threshold(coeffs[i], value=np.std(coeffs[i]) * 0.5, mode='soft')\n",
        "\n",
        "    # Signal reconstruction\n",
        "    clean_signal = pywt.waverec(coeffs, wavelet)\n",
        "    return clean_signal[:len(signal)]\n",
        "\n",
        "# Function for segmentation by time\n",
        "def segment_signal_by_time(signal, sampling_rate, segment_duration=5):\n",
        "    segment_length = segment_duration * sampling_rate\n",
        "    num_segments = len(signal) // segment_length\n",
        "    segments = [\n",
        "        signal[i * segment_length:(i + 1) * segment_length]\n",
        "        for i in range(num_segments)\n",
        "    ]\n",
        "    return np.array(segments)\n",
        "\n",
        "# Function for feature extraction\n",
        "def extract_features(segment, fs):\n",
        "    peaks, properties = find_peaks(segment, height=0.5)\n",
        "    rr_intervals = np.diff(peaks) / fs\n",
        "\n",
        "    # Time-domain features\n",
        "    rMSSD = np.sqrt(np.mean(np.square(np.diff(rr_intervals)))) if len(rr_intervals) > 1 else 0\n",
        "    PRR50 = np.sum(np.abs(np.diff(rr_intervals)) > 0.05) / len(rr_intervals) if len(rr_intervals) > 1 else 0\n",
        "    PRR20 = np.sum(np.abs(np.diff(rr_intervals)) > 0.02) / len(rr_intervals) if len(rr_intervals) > 1 else 0\n",
        "    SDSD = np.std(np.diff(rr_intervals)) if len(rr_intervals) > 1 else 0\n",
        "    SDRR = np.std(rr_intervals) if len(rr_intervals) > 0 else 0\n",
        "\n",
        "    # Frequency-domain features\n",
        "    freqs, power = welch(segment, fs=fs)\n",
        "    low_freq = np.sum(power[(freqs >= 0.1) & (freqs < 0.5)])\n",
        "    mid_freq = np.sum(power[(freqs >= 0.5) & (freqs < 15)])\n",
        "    high_freq = np.sum(power[(freqs >= 15) & (freqs < 40)])\n",
        "\n",
        "    # Poincaré metrics\n",
        "    if len(rr_intervals) > 1:\n",
        "        sd1 = np.std(np.diff(rr_intervals)) / np.sqrt(2)\n",
        "        sd2 = np.std(rr_intervals) / np.sqrt(2)\n",
        "    else:\n",
        "        sd1, sd2 = 0, 0\n",
        "\n",
        "    # Additional features: Min/Max/Average heart rate\n",
        "    if len(rr_intervals) > 0:\n",
        "        heart_rates = 60 / rr_intervals\n",
        "        min_hr = np.min(heart_rates)\n",
        "        max_hr = np.max(heart_rates)\n",
        "        avg_hr = np.mean(heart_rates)\n",
        "    else:\n",
        "        min_hr, max_hr, avg_hr = 0, 0, 0\n",
        "\n",
        "    # Additional features: Average peak amplitudes (P/Q/R/S/T)\n",
        "    peak_amplitudes = properties[\"peak_heights\"] if \"peak_heights\" in properties else []\n",
        "    avg_peak_amplitude = np.mean(peak_amplitudes) if len(peak_amplitudes) > 0 else 0\n",
        "\n",
        "    # Additional features: Average peak delays (PQ/QR/RS/ST)\n",
        "    if len(peaks) >= 5:\n",
        "        pq_delay = np.mean(peaks[1] - peaks[0]) / fs\n",
        "        qr_delay = np.mean(peaks[2] - peaks[1]) / fs\n",
        "        rs_delay = np.mean(peaks[3] - peaks[2]) / fs\n",
        "        st_delay = np.mean(peaks[4] - peaks[3]) / fs\n",
        "    else:\n",
        "        pq_delay, qr_delay, rs_delay, st_delay = 0, 0, 0, 0\n",
        "\n",
        "    # Combine features\n",
        "    features = [\n",
        "        rMSSD, PRR50, PRR20, SDSD, SDRR,\n",
        "        low_freq, mid_freq, high_freq,\n",
        "        sd1, sd2, min_hr, max_hr, avg_hr,\n",
        "        avg_peak_amplitude, pq_delay, qr_delay, rs_delay, st_delay\n",
        "    ]\n",
        "\n",
        "    return features\n",
        "\n",
        "# Updated preprocess_data Function\n",
        "def preprocess_data(all_data, sampling_rate=400, segment_duration=5):\n",
        "    X_segments, X_features, y = [], [], []\n",
        "    label_mapping = {\"A\": 1, \"N\": 0, \"V\": 2}\n",
        "\n",
        "    for folder, files in all_data.items():\n",
        "        if folder not in label_mapping:\n",
        "            print(f\"Ignoring unknown folder: {folder}\")\n",
        "            continue\n",
        "\n",
        "        label = label_mapping[folder]\n",
        "        print(f\"Processing folder: {folder}, number of files: {len(files)}\")\n",
        "        for mat_file_name, mat_data in files:\n",
        "            print(f\"  Processing file: {mat_file_name}\")\n",
        "            if 'ECG' in mat_data:\n",
        "                try:\n",
        "                    ecg_signal = mat_data['ECG']\n",
        "\n",
        "                    # Debugging content\n",
        "                    print(f\"    Data type: {type(ecg_signal)}, Size: {np.shape(ecg_signal)}\")\n",
        "\n",
        "                    # Unpacking and validating\n",
        "                    if isinstance(ecg_signal, np.ndarray):\n",
        "                        # Handle arrays of size (5, 1)\n",
        "                        if ecg_signal.ndim == 2 and ecg_signal.shape[1] == 1:\n",
        "                            ecg_signal = ecg_signal.flatten()\n",
        "\n",
        "                        # Handle nested structures\n",
        "                        if ecg_signal.dtype == 'O':\n",
        "                            ecg_signal = ecg_signal[0].flatten()\n",
        "\n",
        "                    # Validation: ensure data is numeric\n",
        "                    if not isinstance(ecg_signal, np.ndarray) or not np.issubdtype(ecg_signal.dtype, np.number):\n",
        "                        print(f\"    Invalid data in file: {mat_file_name}\")\n",
        "                        continue\n",
        "\n",
        "                    # Filtering\n",
        "                    # 1. Apply wavelet denoising\n",
        "                    ecg_signal = wavelet_filter(ecg_signal, wavelet='db6', level=5)\n",
        "\n",
        "                    # 2. Apply high-pass filter to remove baseline wander\n",
        "                    ecg_signal = butter_highpass_filter(ecg_signal, cutoff=0.5, fs=sampling_rate, order=4)\n",
        "\n",
        "                    # 3. Apply low-pass filter to remove high-frequency noise\n",
        "                    ecg_signal = butter_lowpass_filter(ecg_signal, cutoff=50, fs=sampling_rate, order=4)\n",
        "\n",
        "                    # Augmentation: Only for class 0 (\"N\")\n",
        "                    augmented_signals = [ecg_signal]\n",
        "                    if label == 0:\n",
        "                        augmented_signals.append(-ecg_signal)\n",
        "\n",
        "                    for aug_signal in augmented_signals:\n",
        "                        segments = segment_signal_by_time(aug_signal, sampling_rate, segment_duration)\n",
        "\n",
        "                        for segment in segments:\n",
        "                            features = extract_features(segment, fs=sampling_rate)\n",
        "                            X_segments.append(segment)\n",
        "                            X_features.append(features)\n",
        "                            y.append(label)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    Error processing file {mat_file_name}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    return np.array(X_segments), np.array(X_features), np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vib8tiTf9FZ5"
      },
      "outputs": [],
      "source": [
        "# --- Importing Libraries ---\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv1D, MaxPooling1D, Dense, Dropout, Input, Flatten, Concatenate, Reshape, BatchNormalization,\n",
        "    Bidirectional, LSTM, Layer\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "# --- Attention Layer Class ---\n",
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
        "                                 initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
        "                                 initializer=\"zeros\")\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "\n",
        "# --- Loading and Processing Data ---\n",
        "all_data = load_ecg_database(extract_path)\n",
        "X_segments, X_features, y = preprocess_data(all_data)\n",
        "\n",
        "# --- Data Splitting ---\n",
        "X_segments_train, X_segments_temp, X_features_train, X_features_temp, y_train, y_temp = train_test_split(\n",
        "    X_segments, X_features, y, test_size=0.3, random_state=42\n",
        ")\n",
        "X_segments_val, X_segments_test, X_features_val, X_features_test, y_val, y_test = train_test_split(\n",
        "    X_segments_temp, X_features_temp, y_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_features_train = scaler.fit_transform(X_features_train)\n",
        "X_features_val = scaler.transform(X_features_val)\n",
        "X_features_test = scaler.transform(X_features_test)\n",
        "\n",
        "# Model Architecture\n",
        "input_shape = (X_segments_train.shape[1], 1)\n",
        "\n",
        "input_layer = Input(shape=input_shape)\n",
        "conv1 = Conv1D(32, 5, activation=\"relu\", kernel_regularizer=l2(0.01))(input_layer)\n",
        "conv1 = BatchNormalization()(conv1)\n",
        "pool1 = MaxPooling1D(2)(conv1)\n",
        "\n",
        "conv2 = Conv1D(64, 5, activation=\"relu\", kernel_regularizer=l2(0.01))(pool1)\n",
        "conv2 = BatchNormalization()(conv2)\n",
        "pool2 = MaxPooling1D(2)(conv2)\n",
        "\n",
        "conv3 = Conv1D(128, 3, activation=\"relu\", kernel_regularizer=l2(0.01))(pool2)\n",
        "conv3 = BatchNormalization()(conv3)\n",
        "pool3 = MaxPooling1D(2)(conv3)\n",
        "\n",
        "reshape = Reshape((-1, 128))(pool3)\n",
        "bi_lstm = Bidirectional(LSTM(64, return_sequences=True, dropout=0.4))(reshape)\n",
        "attention = Attention()(bi_lstm)\n",
        "\n",
        "input_features = Input(shape=(X_features_train.shape[1],))\n",
        "dense_features = Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01))(input_features)\n",
        "dense_features = BatchNormalization()(dense_features)\n",
        "dense_features = Dropout(0.4)(dense_features)\n",
        "\n",
        "merged = Concatenate()([attention, dense_features])\n",
        "dense1 = Dense(128, activation=\"relu\", kernel_regularizer=l2(0.01))(merged)\n",
        "dense1 = BatchNormalization()(dense1)\n",
        "dropout1 = Dropout(0.5)(dense1)\n",
        "\n",
        "output_layer = Dense(len(np.unique(y)), activation=\"softmax\")(dropout1)\n",
        "\n",
        "# Compiling the Model\n",
        "model = Model(inputs=[input_layer, input_features], outputs=output_layer)\n",
        "model.compile(optimizer=Adam(learning_rate=0.0003), loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, min_delta=0.001)\n",
        "\n",
        "# Training\n",
        "history = model.fit(\n",
        "    [X_segments_train, X_features_train],\n",
        "    y_train,\n",
        "    validation_data=([X_segments_val, X_features_val], y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[lr_scheduler, early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluation\n",
        "test_loss, test_acc = model.evaluate([X_segments_test, X_features_test], y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "8fTbgpXsAVO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Definition of class labels\n",
        "class_labels = [\"N\", \"A\", \"V\"]\n",
        "\n",
        "# Binarization of labels for multi-class ROC curve\n",
        "y_test_bin = label_binarize(y_test, classes=np.unique(y_train))\n",
        "y_pred_prob = model.predict([X_segments_test, X_features_test])\n",
        "\n",
        "# Generating the ROC curve for each class\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, class_name in enumerate(class_labels):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_prob[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"Class {class_name} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "# Central axis\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Predictions on test data\n",
        "y_pred = model.predict([X_segments_test, X_features_test]).argmax(axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_train))\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plotting the model learning curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title(\"Model Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title(\"Model Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(y_test, y_pred, target_names=class_labels)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)\n",
        "\n",
        "\n",
        "# Compute accuracy, precision, recall, and F1-score for each class\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "for i, class_name in enumerate(class_labels):\n",
        "    precision = precision_score(y_test, y_pred, labels=[i], average='micro')\n",
        "    recall = recall_score(y_test, y_pred, labels=[i], average='micro')\n",
        "    f1 = f1_score(y_test, y_pred, labels=[i], average='micro')\n",
        "    print(f\"Class {class_name}: Precision = {precision:.2f}, Recall = {recall:.2f}, F1-Score = {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "P9mlyDUG37bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Real-time"
      ],
      "metadata": {
        "id": "IaZutiu772zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to save the scaler\n",
        "scaler_path = \"saved_model/scaler.pkl\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(scaler_path), exist_ok=True)\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print(f\"Scaler saved at {scaler_path}\")\n"
      ],
      "metadata": {
        "id": "jcjwt2cP710U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to save the model\n",
        "save_path = \"saved_model/arrhythmia_model.keras\"\n",
        "\n",
        "# Save the model\n",
        "model.save(save_path)\n",
        "print(f\"Model saved at {save_path}\")"
      ],
      "metadata": {
        "id": "pBUHUm_N8Vd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pywt\n",
        "from scipy.signal import butter, filtfilt, find_peaks, welch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Filtering\n",
        "def butter_lowpass_filter(data, cutoff, fs, order=4):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = cutoff / nyquist\n",
        "    b, a = butter(order, low, btype='low')\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def butter_highpass_filter(data, cutoff, fs, order=4):\n",
        "    nyquist = 0.5 * fs\n",
        "    high = cutoff / nyquist\n",
        "    b, a = butter(order, high, btype='high')\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def wavelet_filter(signal, wavelet='db6', level=5):\n",
        "    \"\"\"\n",
        "    Filtering the signal using wavelet transform.\n",
        "    \"\"\"\n",
        "    coeffs = pywt.wavedec(signal, wavelet=wavelet, level=level)\n",
        "\n",
        "    # Zeroing high-level details (high-frequency noise)\n",
        "    for i in range(1, len(coeffs)):\n",
        "        coeffs[i] = pywt.threshold(coeffs[i], value=np.std(coeffs[i]) * 0.5, mode='soft')\n",
        "\n",
        "    # Signal reconstruction\n",
        "    clean_signal = pywt.waverec(coeffs, wavelet)\n",
        "    return clean_signal[:len(signal)]\n",
        "\n",
        "\n",
        "# Feature extraction\n",
        "def extract_features(segment, fs):\n",
        "    peaks, properties = find_peaks(segment, height=0.5)\n",
        "    rr_intervals = np.diff(peaks) / fs\n",
        "\n",
        "    # Time-domain features\n",
        "    rMSSD = np.sqrt(np.mean(np.square(np.diff(rr_intervals)))) if len(rr_intervals) > 1 else 0\n",
        "    PRR50 = np.sum(np.abs(np.diff(rr_intervals)) > 0.05) / len(rr_intervals) if len(rr_intervals) > 1 else 0\n",
        "    PRR20 = np.sum(np.abs(np.diff(rr_intervals)) > 0.02) / len(rr_intervals) if len(rr_intervals) > 1 else 0\n",
        "    SDSD = np.std(np.diff(rr_intervals)) if len(rr_intervals) > 1 else 0\n",
        "    SDRR = np.std(rr_intervals) if len(rr_intervals) > 0 else 0\n",
        "\n",
        "    # Frequency-domain features\n",
        "    freqs, power = welch(segment, fs=fs)\n",
        "    low_freq = np.sum(power[(freqs >= 0.1) & (freqs < 0.5)])\n",
        "    mid_freq = np.sum(power[(freqs >= 0.5) & (freqs < 15)])\n",
        "    high_freq = np.sum(power[(freqs >= 15) & (freqs < 40)])\n",
        "\n",
        "    # Poincaré metrics\n",
        "    if len(rr_intervals) > 1:\n",
        "        sd1 = np.std(np.diff(rr_intervals)) / np.sqrt(2)\n",
        "        sd2 = np.std(rr_intervals) / np.sqrt(2)\n",
        "    else:\n",
        "        sd1, sd2 = 0, 0\n",
        "\n",
        "    # Additional features: Min/Max/Average heart rate\n",
        "    if len(rr_intervals) > 0:\n",
        "        heart_rates = 60 / rr_intervals\n",
        "        min_hr = np.min(heart_rates)\n",
        "        max_hr = np.max(heart_rates)\n",
        "        avg_hr = np.mean(heart_rates)\n",
        "    else:\n",
        "        min_hr, max_hr, avg_hr = 0, 0, 0\n",
        "\n",
        "    # Additional features: Average peak amplitudes (P/Q/R/S/T)\n",
        "    peak_amplitudes = properties[\"peak_heights\"] if \"peak_heights\" in properties else []\n",
        "    avg_peak_amplitude = np.mean(peak_amplitudes) if len(peak_amplitudes) > 0 else 0\n",
        "\n",
        "    # Additional features: Average peak delays (PQ/QR/RS/ST)\n",
        "    if len(peaks) >= 5:\n",
        "        pq_delay = np.mean(peaks[1] - peaks[0]) / fs\n",
        "        qr_delay = np.mean(peaks[2] - peaks[1]) / fs\n",
        "        rs_delay = np.mean(peaks[3] - peaks[2]) / fs\n",
        "        st_delay = np.mean(peaks[4] - peaks[3]) / fs\n",
        "    else:\n",
        "        pq_delay, qr_delay, rs_delay, st_delay = 0, 0, 0, 0\n",
        "\n",
        "    # Combine features\n",
        "    return [\n",
        "        rMSSD, PRR50, PRR20, SDSD, SDRR,\n",
        "        low_freq, mid_freq, high_freq,\n",
        "        sd1, sd2, min_hr, max_hr, avg_hr,\n",
        "        avg_peak_amplitude, pq_delay, qr_delay, rs_delay, st_delay\n",
        "    ]\n",
        "\n",
        "# Segmenting the signal into 5-second fragments\n",
        "def segment_signal_by_time(signal, sampling_rate, segment_duration=5):\n",
        "    segment_length = segment_duration * sampling_rate\n",
        "    num_segments = len(signal) // segment_length\n",
        "\n",
        "    segments = [\n",
        "        signal[i * segment_length:(i + 1) * segment_length]\n",
        "        for i in range(num_segments)\n",
        "    ]\n",
        "\n",
        "    # Handling remaining samples (padding the last segment)\n",
        "    remaining_samples = len(signal) % segment_length\n",
        "    if remaining_samples > 0:\n",
        "        last_segment = signal[-remaining_samples:]\n",
        "        # Padding to 2000 samples (repeating the last sample)\n",
        "        if len(last_segment) < 2000:\n",
        "            padding_length = 2000 - len(last_segment)\n",
        "            last_segment = np.pad(last_segment, (0, padding_length), mode='constant', constant_values=0)\n",
        "        segments.append(last_segment)\n",
        "\n",
        "    return np.array(segments)\n",
        "\n",
        "\n",
        "# Real-time signal processing\n",
        "def process_real_time_signal(signal, model, scaler, sampling_rate=400, segment_duration=5):\n",
        "    try:\n",
        "        # Processing the signal\n",
        "        ecg_signal = signal\n",
        "        ecg_signal = butter_lowpass_filter(ecg_signal, cutoff=50.0, fs=sampling_rate)  # Low-pass filter\n",
        "        ecg_signal = butter_highpass_filter(ecg_signal, cutoff=0.5, fs=sampling_rate)  # High-pass filter\n",
        "        ecg_signal = wavelet_filter(ecg_signal)  # Wavelet filter\n",
        "\n",
        "        # Segmenting the signal into 5-second fragments\n",
        "        segments = segment_signal_by_time(ecg_signal, sampling_rate, segment_duration)\n",
        "\n",
        "        # Store prediction results for each segment\n",
        "        class_mapping = {0: \"N\", 1: \"A\", 2: \"V\"}\n",
        "        predictions = []\n",
        "\n",
        "        # Processing each segment\n",
        "        for i, segment in enumerate(segments):\n",
        "            segment = segment.reshape(1, -1, 1)\n",
        "\n",
        "            if segment.shape != (1, 2000, 1):\n",
        "                continue\n",
        "\n",
        "            features = extract_features(segment.flatten(), fs=sampling_rate)\n",
        "            features = scaler.transform([features])\n",
        "\n",
        "            prediction = model.predict([segment, features])\n",
        "            predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "\n",
        "            predictions.append((i, predicted_class))\n",
        "\n",
        "        # Display ECG signal plot with labeled segments\n",
        "        plt.figure(figsize=(30, 10))\n",
        "        plt.plot(np.arange(len(ecg_signal)) / sampling_rate, ecg_signal, label='ECG Signal', color='red')\n",
        "\n",
        "        # Adding dashed lines for segments\n",
        "        for i, predicted_class in predictions:\n",
        "            start_idx = i * 2000\n",
        "            end_idx = (i + 1) * 2000\n",
        "            plt.axvline(x=start_idx / sampling_rate, color='k', linestyle='--', linewidth=1)\n",
        "            plt.axvline(x=end_idx / sampling_rate, color='k', linestyle='--', linewidth=1)\n",
        "\n",
        "            # Adding label above segment\n",
        "            class_label = class_mapping[predicted_class]\n",
        "            plt.text((start_idx + end_idx) / (2 * sampling_rate), 0.7 * np.max(ecg_signal), class_label, ha='center', va='bottom', fontsize=12, color='black')\n",
        "\n",
        "        # Adding title and labels\n",
        "        plt.title(\"Real-Time ECG Signal with Predicted Arrhythmias\", fontsize=23)\n",
        "        plt.xlabel('Time (s)', fontsize=20)\n",
        "        plt.ylabel('Amplitude', fontsize=20)\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in signal processing: {e}\")\n",
        "\n",
        "\n",
        "# Loading model and scaler\n",
        "model_path = \"saved_model/arrhythmia_model.keras\"\n",
        "model = load_model(model_path, custom_objects={\"Attention\": Attention})\n",
        "\n",
        "scaler_path = \"saved_model/scaler.pkl\"\n",
        "scaler = joblib.load(scaler_path)\n",
        "\n",
        "# Load multiple .mat files\n",
        "file_paths = [...]  # List of file paths\n",
        "\n",
        "# Process the combined signal in real time\n",
        "process_real_time_signal(np.array(ecg_signal_combined), model, scaler)\n"
      ],
      "metadata": {
        "id": "0-jbvB2IMUoV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}